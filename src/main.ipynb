{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Question\n",
    "Large Language Models perform really well on natural language generation tasks, sometimes equally or surpassing human abilities. Since GPT, natural language processing has seen many advancements, and ChatGPT showcased the power of LLMs on variety of tasks like question answering, generating long texts, and even simple logical reasoning. But, the answers were based out of the parametric memory of the model which was frozen, i.e., it could not update or fetch relevant data. Many a times the LLMs would invent results and output them: they would *hallucinate*. **Is there a way to make large language models fetch relevant information and generate accurate answers?**\n",
    "\n",
    "## The Dataset\n",
    "We are utilizing 3 chapters (chapter 1, 2, and 7) of the course textbook 'Artificial Intelligence: A Modern Approach' and a couple of slides from Prof. Keogh and Prof. LePendu as our additional context.\n",
    "\n",
    "## The Method\n",
    "To achieve our objective of eliminating hallucination and boost relevancy, we utilize Llama-Index, LangChain, and Ollama to build a RAG pipeline. Llama-Index is used to index the documents and store them in a vector database. LangChain serves the local Ollama model of Llama 2 7B. The chapters from the textbook and the slides are index and stored in a vector database which are then retrieved based on the similarity with the query passed to the language model. We have used OpenAI's state-of-the-art text-embedding-ada-002 embedding model through their API. \n",
    "\n",
    "## Our hypothesis\n",
    "We feel that supplementing the LLM with additional context can boost the relevance of the answer and stop it from hallucinating. The evaluation is done using the RAGAS approach where we measure how relevant and consistent the generated result is to the context and the query.   \n",
    "\n",
    "## Why bother?\n",
    "As the use of LLM is rising, one LLM cannot know everything in this universe. It is important to have specific LLM for a specific purpose. It is also important to have fresh data for the LLM to access and output relevant and factually correct results. The approach of RAG seems promising to alleviate most of the problems related to LLMs generation and correct usage can boost the utilization of the language models in various fields. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Jupyter Notebook for the CS205 Final Project\n",
    "*This only works if run locally after completing all the installations mentioned in the README file of the project*\n",
    "\n",
    "1. In this project we will explore a usecase of Large Language Models\n",
    "2. We will start with how to use an LLM through HuggingFace, and explain some of the basic concepts behind an LLM. \n",
    "3. Once we have a good understading of how to use an LLM for generating text, we will explore Retrieval Augmented Generation (RAG). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this project we have used Llama 2 7 Billion paramter model with OpenAI's text-embed-002 embedding model. Llama 2 7B was served locally by Ollama. We have used Llama Index and LangChain to interact with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data store is at the root of the project directory with the name 'data'. Create a data repository before running the indexing and query cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's understand how to use an LLM using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Import HuggingFace Transformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Meta's OPT LLM with 1.3 billion parameters. This is quite a small model compared to the SOTA like GPT4V, etc.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open Pre-trained Transformer (OPT) is a collection of decoder-only transformer developed by Meta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "input_text = 'I like CS205 Artificial Intelligence course, because'\n",
    "tok_input = tokenizer(input_text, return_tensors='pt', add_special_tokens=True, truncation=True) #Create tokens from the given input and return PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "generated_output = model.generate(**tok_input, \n",
    "                                  max_new_tokens=200, \n",
    "                                  return_dict_in_generate=True, \n",
    "                                  do_sample=True) #Generation is deterministic. \n",
    "                                                  #To use top-k sampling, set do_sample=True to get different responses in each generation\n",
    "                                                  #Set do_sample=False to have a deterministic generation each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like CS205 Artificial Intelligence course, because of the breadth of material covered and the fact that it is an introductory course, so the material is \"fresh\". I think its a good course. (Yes, I do agree that I like the math, and i do agree that in all CS courses, the first course or two are usually the hardest ones).\n",
      "Do you think you would still like it had you not taken a calculus course with higher math content\n"
     ]
    }
   ],
   "source": [
    "decoded_output = tokenizer.batch_decode(generated_output.sequences, skip_special_tokens=True)[0]\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an embedding?\n",
    "\n",
    "Embedding is a numerical representation of the text. The words in the vocabulary are mapped to a set of integers. These integers are then converted into another mathematical representation. This 'embedding' vector is of shape (n_tokens x embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Life is sweet and sugary, but eating ice cream on Everest is long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize1 = {s: i for i, s in enumerate(sorted(sentence.replace(',', '').split()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  8, 12,  2, 11,  3,  5,  6,  4, 10,  0,  8,  9])\n"
     ]
    }
   ],
   "source": [
    "sentence_vec = torch.tensor([tokenize1[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence1 is tokenized as shown in the output above. The words are mapped to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12823fb70>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(len(sentence_vec), 10)\n",
    "embedded_sentence = embed(sentence_vec).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence \"Life is sweet and sugary, but eating ice cream on Everest is long\" now has a representation like shown below. It is converted from text to a form which the machine understands. This embedding is the input to a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 10])\n",
      "tensor([[-0.4223, -1.1036,  0.8398, -1.0029,  0.5253,  0.9389, -0.0306, -0.0894,\n",
      "         -0.1965, -0.9713],\n",
      "        [ 0.1419,  0.3696, -0.0174, -0.9575,  1.2968,  0.6833,  0.2154,  0.3307,\n",
      "         -2.1467, -1.7984],\n",
      "        [-0.9124, -1.4065,  1.3834,  0.0324,  0.0040,  0.3480,  1.7276, -2.5230,\n",
      "          0.2561,  1.0097],\n",
      "        [ 0.2790, -0.7587,  0.5473,  0.4301,  0.8558,  1.6098, -1.1893,  1.1677,\n",
      "          0.6220,  2.5737],\n",
      "        [ 0.8243, -0.4808, -0.1200,  0.4884,  1.1051, -0.5454, -0.2115, -0.2708,\n",
      "         -0.0830, -0.2453],\n",
      "        [-0.6239, -1.2965, -0.4382,  0.3265, -1.5786, -1.3995,  0.2425,  0.3648,\n",
      "          1.3119, -0.2825],\n",
      "        [ 0.7047, -0.2722,  0.0781, -0.1134, -0.7817,  0.8967, -0.4619, -1.5539,\n",
      "         -0.3338,  0.2405],\n",
      "        [-0.0334,  1.5544,  0.3418, -1.5768, -0.6933,  1.7409,  0.2698,  0.9595,\n",
      "          0.7744,  1.8721],\n",
      "        [ 1.7524, -0.2135,  0.4095,  0.0465,  0.5468,  1.1478, -0.3339, -0.6653,\n",
      "          0.9376, -0.9225],\n",
      "        [ 0.6413,  0.5291,  1.2251,  0.5123,  0.4765, -1.4498,  1.5446,  1.0394,\n",
      "         -0.3804,  0.9701],\n",
      "        [-2.2121, -0.3780,  0.9838, -1.0895, -0.6378, -0.6031, -1.8627,  1.1482,\n",
      "          0.2781, -0.9621],\n",
      "        [ 0.1419,  0.3696, -0.0174, -0.9575,  1.2968,  0.6833,  0.2154,  0.3307,\n",
      "         -2.1467, -1.7984],\n",
      "        [-0.6822, -0.5191,  0.1669, -1.2620,  0.1699,  1.1698,  1.0875, -0.1029,\n",
      "          0.8604,  0.2078]])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence.shape)\n",
    "print(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I feel like driving car today\"\n",
    "sentence2 = \"I think I want to drive today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = model.encode(sentence1)\n",
    "embedding2 = model.encode(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a cosine similarity of 2 embeddings tells us how similar the vectors are semantically. This is particularly useful in our application where the query is matched with documents using a similarity function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7208]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.pytorch_cos_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closer the value is to 1, the more semantically closer the embeddings are. We can explore the similarity with other sentences by modifying the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation \n",
    "\n",
    "It is a technique in generative AI to boost the knowledge of an LLM. The LLM parameters are learned and not updated to the current information, so a specialized database of knowledge (could be private) is created for the LLM is access. This is a non-parametric memory, i.e, this information is not stored in the learned paramaters of the LLM. RAG combines retrieval with generation for content generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does an RAG work?\n",
    "\n",
    "1. Retrieval: The model retrieves a set of top-k relevant documents that act as additional context for the query. Since the documents are stored in the database in the form of embeddings, the model can perform similarity searches in retrieval\n",
    "\n",
    "2. Generation: Once the documents are retrieved, this serves as additional context along with the original input. The generative model can now use both the original input and retrieved context to generated the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing RAG with Llama Index using ChromaDB as the vector database. Ollama is used to serve Llama 2 locally\n",
    "\n",
    "The data can be accessed at this link. Add this folder to the root of the project directory\n",
    "\n",
    "https://drive.google.com/drive/folders/10wzRErO4Zlj6L3bLSqh9QtTLDkaUOuxS?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, download_loader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = dotenv_values('../.env')[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key # For RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the embedding\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "#embed_model = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\") #Local Llama 2 embedding model\n",
    "embed_model = OpenAIEmbedding() #Using OpenAI's text-embed-002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text-embedding-ada-002 is a powerful embedding model released by OpenAI. Like we discussed, an embedding represents a text mathematically in n-dimensions, and the distance between the embeddings can measure the similarity between the sentences or words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"aiprof\"\n",
    "SLIDE_COLLECTION = 'slides'\n",
    "PATH = '../chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "db = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/AIMA/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_reader = download_loader(\"PptxReader\")\n",
    "loader = slides_reader()\n",
    "slides = []\n",
    "\n",
    "for file in os.listdir(\"../data/slides/\"):\n",
    "    if file.endswith(\".pptx\"):\n",
    "        slides += loader.load_data(\"../data/slides/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = documents + slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    information, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db2.get_or_create_collection(COLLECTION)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index2 = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Turing test is a check to see if a computer can behave like a human. It includes visual signals that the interrogator cannot test the TOTAL Turing Test subject's perceptual abilities, as well as the opportunity for the interrogator to pass physical objects through a hatch. To pass the total Turing Test, the computer will need computer vision to perceive objects and robotics to move them around. Within AI, there has not been a big effort to try to pass the Turing test. The main problem comes when AI programs have to interact with people, such as when an expert system explains how it came to its diagnosis or a natural language processing system has a dialogue with a user. These programs must behave according to certain normal conventions of human interaction in order to make themselves understood. The underlying representation and reasoning in such a system may or may not be based on a human model. Thinking humanly: The cognitive modeling approach If we are going to say that a given program thinks like a human, we must have some way of determining how humans think. We need to get inside the actual workings of human minds. There are two ways to do this: through introspection (trying to catch our own thoughts as they go by) or through psychological experiments. Once we have a suf®ciently precise theory of the mind, it becomes possible to express the theory as a computer program. If the program's input/output and timing behave in a way that matches human behavior, that is evidence that some of the program's mechanisms may also be operating in humans. For example, Newell and Simon, who developed GPS (the General Problem Solver), were not content to have their program correctly solve problems. They were more concerned with comparing the trace of its reasoning steps to traces of human subjects solving the same problems. This is in contrast to other researchers of the same time (such as Wang in 1960), who were concerned with getting the right answers regardless of how humans might do it. The interdisciplinary field of cognitive science brings together computer models from AI and experimental techniques from cognitive science psychology to try to construct precise and testable theories of the workings of the human mind. Although cognitive science is a fascinating ®eld in itself, we are not going to be discussing it all that much in this book. We will occasionally comment on similarities or differences between AI techniques and human cognition. Real cognitive science, however, is necessarily based on experimental investigation of actual humans or animals, and we assume that the reader only has access to a computer for experimentation. We will simply notethat AI and cognitivescience continueto fertilize each other, especially in the areas of vision, natural language, and learning. The history of psychological theories of cognition is briefly covered on page 12. Thinking rationally: The laws of thought approach The Greek philosopher Aristotle was one of the ®rst to attempt to codify right thinking, that is, irrefutable reasoning processes. His famous sylogisms provided patterns for argument structures SYLLOGISMS that always gave correct conclusions given correct premises. For example, ªSocrates is a man; Arti®cial Intelligence: AModernApproach by Stuart Russell and Peter Norvig,c 1995Prentice-Hall, Inc.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"What is the turing test?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial search is a type of search algorithm that involves assuming there is an enemy entity that can potentially thwart the agent's goals. The algorithm relaxes the assumption of full control over the environment and considers the possibility of an opponent who can make moves that affect the outcome of the game.\n",
      "\n",
      "In adversarial search, the algorithm considers different types of games, including cooperative and competitive games. For competitive games, the algorithm identifies the initial state, a set of operators, a terminal test, and a utility function. The utility function evaluates each node in terms of how good it is for each player, with positive values indicating states advantageous for Max and negative values indicating states advantageous for Min.\n",
      "\n",
      "The algorithm then generates the game tree down to the terminal nodes and applies the utility function to the terminal nodes. It recursively passes up the backed-up values until reaching the initial state, and the minimum score for Max is the final answer.\n",
      "\n",
      "Adversarial search can be applied to various fields beyond games, including economics, business, politics, and war. However, the algorithm does not consider such considerations in detail.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"What is Adversarial Search?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"Generate 2 concise questions about rational agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the difference between a performance measure and a utility function in the context of rational agents?',\n",
       " '2. Choose a familiar domain, and write a page description of an agent for that environment. Determine the most appropriate agent architecture for this domain (table lookup, simple reactive, goal-based, or utility-based)?']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.response.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"When is the final project due\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, the final project is due on November 30, 2023 (tentative).\n"
     ]
    }
   ],
   "source": [
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We have been quite successful at getting what looks like accurate results, but lets run some evaluations on the generated text. Inspired from the RAGAS paper (https://arxiv.org/pdf/2309.15217v1.pdf), there is a ragas python library which runs evaluations on the RAG generated text. \n",
    "\n",
    "**Retrieval Augmentation Generated Assessment** evaluates a RAG generated text without human interference. Even after utilizing RAG we wouldn't be sure if the system hallucinated the generation. The paper proposes a suite of metrics that evaluates RAG.\n",
    "\n",
    "*Faithfulness*: Measures the factual consistency of the generated answer. It is based on the answer and the retrieved context. The generated answer is faithful if all the answers can be deduced from the given context. The score ranges from 0 to 1. Higher the score, the better. \n",
    "\n",
    "*Answer Relevance*: Measures how relevant the answer is to the given prompt. Incomplete and/or redudant answers are given a lower score. The score ranges from 0 to 1.   \n",
    "\n",
    "*Context Relevance*: Similar to answer relevancy, context relevancy measures the relevance of the generated text to the context. The idea is that the context should exlusively contain the information required to answer the query prompt. \n",
    "\n",
    "*Context Precision*: Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "\n",
    "*Context Recall*: Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "Definitions are sourced from https://github.com/explodinggradients/ragas/tree/main/docs/concepts/metrics and the RAGAS paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    harmfulness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llama_index import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\"What is the turing test?\"]\n",
    "\n",
    "eval_answers = [\"The Turing test, originally called the imitation game by Alan Turing in 1950,[2] is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses.\"]\n",
    "eval_answers = [[a] for a in eval_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:00<00:00, 60.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [harmfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(query_engine, metrics, eval_questions, eval_answers) #Takes long to run. 5-10min depending on the number of questions and answers. Might even timeout. \n",
    "#The results of this evaluation will depend on the async library on your local. This is just a demonstration that generation can be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the turing test?</td>\n",
       "      <td>[6 Chapter 1.Introduction\\ntheso-called totalT...</td>\n",
       "      <td>The Turing test is a check to determine whethe...</td>\n",
       "      <td>[The Turing test, originally called the imitat...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   question  \\\n",
       "0  What is the turing test?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [6 Chapter 1.Introduction\\ntheso-called totalT...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The Turing test is a check to determine whethe...   \n",
       "\n",
       "                                       ground_truths  faithfulness  \\\n",
       "0  [The Turing test, originally called the imitat...           0.4   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  harmfulness  \n",
       "0            0.8753                0.0             1.0            0  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated what a large language model is, how to use a model from Huggingface, and discussed what embeddings are. We went further by exploring Retrieval Augment Generation (RAG), a novel method to boost the generation of an LLM. We then evaluated the results using RAGA, a \n",
    "\n",
    "Retrieval Augmented Generation has helped make LLMs more specific and relevant. It has demonstrated that it can reduce hallucination and boost the relevance of the generated text while offering more control and interpretability. Though this method is quite effective, it should be acknowledged that no external source is perfect, devoid of misinformation and bias. We have a long way to go before such models can be used without human interference to verify and interpret the results generated by such methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
