{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Jupyter Notebook for the CS205 Final Project\n",
    "\n",
    "## In this project we will explore a usecase of Large Language Models\n",
    "### We will start with how to use an LLM through HuggingFace, and explain some of the basic concepts behind an LLM. Once we have a good understading of how to use an LLM for generating text, we will explore Retrieval Augmented Generation (RAG). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this project we have used Llama 2 7 Billion paramter model with OpenAI's text-embed-002 embedding model. Llama 2 7B was served locally by Ollama. We have used Llama Index and LangChain to interact with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data store is at the root of the project directory with the name 'data'. Create a data repository before running the indexing and query cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand how to use an LLM using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshavardhank/anaconda3/envs/cs205/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#Import HuggingFace Transformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Meta's OPT LLM with 1.3 billion parameters. This is quite a small model compared to the SOTA like GPT4V, etc.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open Pre-trained Transformer (OPT) is a collection of decoder-only transformer developed by Meta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'I like CS205 Artificial Intelligence course, because'\n",
    "tok_input = tokenizer(input_text, return_tensors='pt', add_special_tokens=True, truncation=True) #Create tokens from the given input and return PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "generated_output = model.generate(**tok_input, \n",
    "                                  max_new_tokens=200, \n",
    "                                  return_dict_in_generate=True, \n",
    "                                  do_sample=True) #Generation is deterministic. \n",
    "                                                  #To use top-k sampling, set do_sample=True to get different responses in each generation\n",
    "                                                  #Set do_sample=False to have a deterministic generation each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(generated_output.sequences, skip_special_tokens=True)[0]\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We saw text generation in the previous subsection. Now lets explore text summarization, a critical usecase of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an embedding?\n",
    "\n",
    "Embedding is a numerical representation of the text. The words in the vocabulary are mapped to a set of integers. These integers are then converted into another mathematical representation. This 'embedding' vector is of shape (n_tokens x embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Life is sweet and sugary, but eating ice cream on Everest is long\"\n",
    "sentence2 = \"The apple pie I eat is very sweet, it will take long to finish eating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize1 = {s: i for i, s in enumerate(sorted(sentence1.replace(',', '').split()))}\n",
    "tokenize2 = {s: i for i, s in enumerate(sorted(sentence2.replace(',', '').split()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  8, 12,  2, 11,  3,  5,  6,  4, 10,  0,  8,  9])\n"
     ]
    }
   ],
   "source": [
    "sentence_vec = torch.tensor([tokenize1[s] for s in sentence1.replace(',', '').split()])\n",
    "print(sentence_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence1 is tokenized as shown in the output above. The words are mapped to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12823fb70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(len(sentence_vec), 10)\n",
    "embedded_sentence = embed(sentence_vec).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence \"Life is sweet and sugary, but eating ice cream in Manali is long\" now has a representation like shown below. It is converted from text to a form which the machine understands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 10])\n",
      "tensor([[ 0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692, -0.0770, -1.0205,\n",
      "         -0.1690,  0.9178],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764],\n",
      "        [-1.4205, -0.2238, -0.2548,  1.1517, -0.0179,  0.4264, -0.7657, -0.0545,\n",
      "         -0.7321,  1.2347],\n",
      "        [ 1.5810,  1.3010,  1.2753, -0.2010,  0.4965, -1.5723,  0.9666, -1.1481,\n",
      "         -1.1589,  0.3255],\n",
      "        [-0.9896,  0.7016, -0.9405, -0.4681, -0.8016, -0.8183, -1.1820, -0.2877,\n",
      "         -0.6043,  0.6002],\n",
      "        [-0.6315, -2.8400, -1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343,\n",
      "         -0.4991, -1.0867],\n",
      "        [-1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,  2.2473, -0.8036,\n",
      "         -0.2808,  0.7697],\n",
      "        [-0.6596, -0.7979,  0.1838,  0.2293,  0.5146,  0.9938, -0.2587, -1.0826,\n",
      "         -0.0444,  1.6236],\n",
      "        [ 0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850,\n",
      "          0.8768,  1.6221],\n",
      "        [-0.0319, -0.4790,  0.7668,  0.0275,  1.9929,  1.3708, -0.5009, -0.2793,\n",
      "         -2.0628,  0.0064],\n",
      "        [ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764],\n",
      "        [ 0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729, -2.2150, -1.3193,\n",
      "         -2.0915,  0.9629]])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence.shape)\n",
    "print(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation is a technique in generative AI to boost the knowledge of an LLM. The LLM parameters are learned and not updated to the current information, so a specialized database of knowledge (could be private) is created for the LLM is access. This is a non-parametric memory, i.e, this information is not stored in the learned paramaters of the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing RAG with Llama Index using ChromaDB as the vector database. Ollama is used to serve Llama 2 locally\n",
    "\n",
    "The data can be accessed at this link. Add this folder to the root of the project directory\n",
    "\n",
    "https://drive.google.com/drive/folders/10wzRErO4Zlj6L3bLSqh9QtTLDkaUOuxS?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, download_loader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = dotenv_values('../.env')[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the embedding\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "#embed_model = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\") #Local Llama 2 embedding model\n",
    "embed_model = OpenAIEmbedding() #Using OpenAI's text-embed-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"aiprof\"\n",
    "SLIDE_COLLECTION = 'slides'\n",
    "PATH = '../chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "db = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/AIMA/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=PATH)\n",
    "chroma_collection = db2.get_or_create_collection(COLLECTION)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index2 = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Turing Test is a theoretical framework proposed by Alan Turing in 1950 to measure a machine's ability to exhibit intelligent behavior comparable to, or indistinguishable from, that of a human. The test involves interrogating a machine (a computer or a robot) via a teletype and assessing whether an human interrogator cannot tell if it is a machine or a human at the other end. The test requires the machine to pass certain cognitive tasks, such as natural language processing, knowledge representation, automated reasoning, and machine learning, sufiiciently well to fool the interrogator. The Turing Test is considered a benchmark for measuring the intelligence of machines and has been the subject of much research and debate in the fi eld of Artificial Intelligence (AI).\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"What is the turing test?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"Generate 2 concise questions about rational agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.response.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying Slides (PPTx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "slides_db = chromadb.PersistentClient(path=PATH)\n",
    "slides_chroma_collection = slides_db.get_or_create_collection(SLIDE_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=slides_chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'abc.ABCMeta'>\n"
     ]
    }
   ],
   "source": [
    "slides_reader = download_loader(\"PptxReader\")\n",
    "loader = slides_reader()\n",
    "\n",
    "print(type(SimpleDirectoryReader))\n",
    "\n",
    "slides = loader.load_data(Path('../data/slides/4_Adversarial_Search.pptx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slides_index = VectorStoreIndex.from_documents(documents=slides, \n",
    "                                               storage_context=storage_context, \n",
    "                                               service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context information provided, the Minimax algorithm is a decision-making algorithm used in game theory and artificial intelligence. It is an optimal algorithm that helps players make the best move possible in a game by evaluating the utility function of each possible move and selecting the one with the highest value. The algorithm works by recursively exploring the game tree down to the terminal nodes, evaluating the utility function of each node, and choosing the move with the highest expected payoff.\n",
      "\n",
      "The Minimax algorithm is guaranteed to find the optimal move in a game, but it has a time complexity that grows exponentially with the depth of the game tree. This means that as the depth of the tree increases, the time required to find the optimal move also increases dramatically. To address this issue, Alpha-Beta pruning is used to reduce the number of nodes that need to be evaluated, making the search process more efficient.\n",
      "\n",
      "In summary, the Minimax algorithm is a decision-making algorithm used in game theory and artificial intelligence that helps players make the best move possible in a game by evaluating the utility function of each possible move and selecting the one with the highest value. It is an optimal algorithm but has a high time complexity, which can be addressed using Alpha-Beta pruning.\n"
     ]
    }
   ],
   "source": [
    "slides_query = slides_index.as_query_engine()\n",
    "resp = slides_query.query(\"What is The Minimax Algorithm?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
